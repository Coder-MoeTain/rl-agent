# Maximum GPU Utilization - CPU-Bound Environment Optimization

## üîç The Problem

Your environment makes **HTTP requests** which are **CPU-bound and blocking**. This creates a bottleneck:

1. **Environment Step** (CPU-bound):
   - Makes HTTP request ‚Üí waits for response
   - Parses HTML with BeautifulSoup
   - Updates NetworkX graph
   - All CPU operations

2. **GPU Training** (GPU-bound):
   - Processes neural network batches
   - Fast when it has data
   - Idle when waiting for environment data

**Result**: GPU sits idle while waiting for environment steps to complete.

## ‚úÖ The Solution

### Strategy: Overlap CPU and GPU Work

Use **many parallel environments** so:
- While GPU processes batch 1, environments 1-32 collect data for batch 2
- While environments wait for HTTP responses, GPU processes previous batch
- GPU stays busy with large batches while environments work in parallel

### Key Optimizations Applied:

1. **32 Parallel Environments** (was 8)
   - More environments = more data collection overlap
   - Uses `SubprocVecEnv` for true parallelism (separate processes)

2. **Very Large Batches** (512, was 256)
   - Keeps GPU busy longer per batch
   - More parallel computation

3. **Very Large Networks** (1024‚Üí1024‚Üí512‚Üí256, was 512‚Üí512‚Üí256)
   - More computation per forward/backward pass
   - Better GPU utilization

4. **Large Rollouts** (8192 steps, was 4096)
   - More data per rollout
   - GPU processes longer while envs collect next rollout

5. **More Epochs** (30, was 20)
   - More computation per rollout
   - GPU stays busy while envs collect data

## üöÄ Usage

### Maximum GPU Script
```bash
python train_max_gpu.py
```

### Updated Standard Scripts
```bash
# Now use 32 parallel environments automatically
python agents/train_single_agent.py
python train_sb3_per.py
```

## üìä Expected Results

### GPU Utilization:
- **Before**: 10-30% (GPU idle waiting for env steps)
- **After**: **80-100%** ‚úÖ (GPU busy with large batches while envs work in parallel)

### Training Speed:
- **Before**: ~50 steps/second
- **After**: **150-300 steps/second** ‚úÖ

### Memory Usage:
- **GPU Memory**: ~4-6 GB (larger networks + batches)
- **System RAM**: Higher (32 parallel processes)

## üîç Monitor GPU Usage

### Real-Time Monitoring:
```bash
# Terminal 1: Training
python train_max_gpu.py

# Terminal 2: Monitor GPU
nvidia-smi -l 1

# Or use monitoring script
python monitor_gpu_usage.py
```

### What to Look For:
- **GPU Utilization**: Should be 80-100%
- **GPU Memory**: Should be 4-6 GB
- **Power Usage**: Should be high (near max)
- **Temperature**: May increase (normal under load)

## ‚öôÔ∏è Configuration Details

### SubprocVecEnv (Critical!)
```python
from stable_baselines3.common.vec_env import SubprocVecEnv

env = SubprocVecEnv([make_env for _ in range(32)], start_method='spawn')
```

**Why**: 
- `DummyVecEnv`: Environments run sequentially (one at a time)
- `SubprocVecEnv`: Environments run in parallel processes (true parallelism)
- **Critical for CPU-bound environments!**

### Hyperparameters:
```python
n_envs = 32          # Many parallel environments
n_steps = 8192       # Large rollout
batch_size = 512     # Very large batches
n_epochs = 30        # Many epochs
net_arch = [1024, 1024, 512, 256]  # Very large network
```

## üéØ Why This Works

### Timeline Example:

**Without Optimization (8 envs, small batches):**
```
Time: 0s    GPU: Process batch 1 (0.5s)
Time: 0.5s  GPU: Idle (waiting for env data)
Time: 1s    Envs: Finish collecting
Time: 1s    GPU: Process batch 2 (0.5s)
Time: 1.5s  GPU: Idle again...
Result: GPU utilization ~30%
```

**With Optimization (32 envs, large batches):**
```
Time: 0s    GPU: Process large batch 1 (2s)
Time: 0s    Envs 1-32: Start collecting data in parallel
Time: 1s    Envs: Finish collecting (while GPU still processing)
Time: 2s    GPU: Finish batch 1, start batch 2 immediately
Time: 2s    Envs: Already collecting batch 3
Result: GPU utilization ~90-100%
```

## üí° Adjustments

### If GPU Memory Error:
```python
n_envs = 16        # Reduce from 32
batch_size = 256   # Reduce from 512
net_arch = [512, 512, 256]  # Smaller network
```

### If Still Low GPU Usage:
```python
n_envs = 64        # Even more environments
batch_size = 1024  # Larger batches (if memory allows)
n_epochs = 40      # More epochs
```

### If System Overloaded:
```python
n_envs = 16        # Fewer processes
# Keep large batches and networks
```

## üîß Troubleshooting

### GPU Still < 50% Usage

**Check:**
1. Are you using `SubprocVecEnv`? (Check output - should say "SubprocVecEnv")
2. How many environments? (Should be 32)
3. Batch size? (Should be 512)
4. Network size? (Should be 1024‚Üí1024‚Üí512‚Üí256)

**Solutions:**
- Increase `n_envs` to 64
- Increase `batch_size` to 1024 (if memory allows)
- Increase `n_epochs` to 40

### Out of Memory Errors

**Solutions:**
- Reduce `n_envs` to 16
- Reduce `batch_size` to 256
- Reduce network size
- Close other applications

### Slow Training

**Possible Causes:**
- Not enough parallel environments
- Batch size too small
- Network too small
- Environment step time (HTTP requests) too slow

**Solutions:**
- Increase parallel environments
- Increase batch size
- Increase network size
- Check network latency to target

## üìà Performance Metrics

### RTX 5080 Expected Performance:

| Metric | Before | After |
|--------|--------|-------|
| **GPU Utilization** | 10-30% | **80-100%** ‚úÖ |
| **Training Speed** | 50 steps/s | **150-300 steps/s** ‚úÖ |
| **GPU Memory** | 500 MB | 4-6 GB |
| **Power Usage** | Low | High (near max) |
| **Throughput** | 1x | **3-6x** ‚úÖ |

## ‚ú® Summary

**The key insight**: CPU-bound environments (HTTP requests) need **many parallel environments** to keep GPU busy.

**Optimizations:**
- ‚úÖ 32 parallel environments (SubprocVecEnv)
- ‚úÖ Very large batches (512)
- ‚úÖ Very large networks (1024‚Üí1024‚Üí512‚Üí256)
- ‚úÖ Large rollouts (8192 steps)
- ‚úÖ Many epochs (30)

**Result**: GPU stays busy processing large batches while environments collect data in parallel! üöÄ


